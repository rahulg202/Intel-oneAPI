{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M6pn7ysA9QBJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Technical/AI ML Model/Dataset/dataset.csv')\n",
        "dataset\n",
        "columns_to_drop = ['Index', 'Air Temperature', 'Water Temperature', 'Month', 'Day', 'Time of Day']\n",
        "dataset = dataset.drop(columns=columns_to_drop)\n",
        "dataset\n",
        "dataset.isnull().sum()\n",
        "columns_to_impute = [\"pH\", \"Iron\", \"Nitrate\", \"Chloride\", \"Lead\", \"Zinc\", \"Turbidity\", \"Fluoride\", \"Copper\", \"Odor\", \"Sulfate\", \"Conductivity\", \"Chlorine\",\"Manganese\", \"Total Dissolved Solids\"]\n",
        "high_frequency_columns = [\"Color\", \"Source\"]\n",
        "for col in high_frequency_columns:\n",
        "    most_frequent_value = dataset[col].mode().values[0]\n",
        "    dataset[col].fillna(most_frequent_value, inplace=True)\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "dataset[columns_to_impute] = imputer.fit_transform(dataset[columns_to_impute])\n",
        "dataset.isnull().sum()\n",
        "unique_values = dataset['Color'].unique()\n",
        "unique_values\n",
        "color = {'Colorless': 0,'Faint Yellow': 1,'Light Yellow': 2,'Near Colorless': 3,'Yellow':4}\n",
        "dataset['Color'] = dataset['Color'].map(color)\n",
        "dataset\n",
        "unique_values = dataset['Source'].unique()\n",
        "unique_values\n",
        "source = {'Stream': 0,'Lake': 1,'River': 2,'Ground': 3,'Spring': 4,'Aquifer': 5,'Reservoir': 6,'Well': 7}\n",
        "dataset['Source'] = dataset['Source'].map(source)\n",
        "dataset\n",
        "X = dataset.iloc[:, 0:-2]\n",
        "y = dataset.iloc[:, -1]\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "_wA56Hyc9UHJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "ZUmFy29y9Whq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_shape, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ml9f_KnW9YUq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNet(X_train_scaled.shape[1])\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "DzsYF3Oa9Zub"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_timept = time.time()"
      ],
      "metadata": {
        "id": "oAVl7_WX9a_4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(X_train_scaled, y_train)\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n",
        "val_data = TensorDataset(X_val, y_val)\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=32)"
      ],
      "metadata": {
        "id": "4AK7uHnC9cJ7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            outputs = model(batch_x)\n",
        "            val_loss += criterion(outputs.squeeze(), batch_y).item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Loss: {val_loss/len(val_loader):.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pxhYoig9duH",
        "outputId": "243eafad-a3ff-4cc1-87ec-b2d1ef7a6c26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0853, Validation Loss: 0.0903\n",
            "Epoch [2/10], Loss: 0.0529, Validation Loss: 0.0897\n",
            "Epoch [3/10], Loss: 0.0877, Validation Loss: 0.0891\n",
            "Epoch [4/10], Loss: 0.0837, Validation Loss: 0.0888\n",
            "Epoch [5/10], Loss: 0.0900, Validation Loss: 0.0889\n",
            "Epoch [6/10], Loss: 0.0623, Validation Loss: 0.0885\n",
            "Epoch [7/10], Loss: 0.1190, Validation Loss: 0.0884\n",
            "Epoch [8/10], Loss: 0.0415, Validation Loss: 0.0885\n",
            "Epoch [9/10], Loss: 0.0546, Validation Loss: 0.0885\n",
            "Epoch [10/10], Loss: 0.0755, Validation Loss: 0.0883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end_timept = time.time()\n",
        "runtimept = end_timept - start_timept\n",
        "print(f\"Total Runtime: {runtimept:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqTc-MdX9fjp",
        "outputId": "51f8aa1b-2f9f-48b5-b16e-e0c68a48e053"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Runtime: 2581.24 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_scaled)\n",
        "    mse = criterion(outputs.squeeze(), y_test)\n",
        "print(f\"Mean Squared Error on Test Set: {mse.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCaHSOyh9gzB",
        "outputId": "b1cf8c96-cfef-4197-f686-894703372464"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on Test Set: 0.08758249878883362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "correct = 0\n",
        "total = y_test.size(0)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_scaled)\n",
        "    diff = torch.abs(outputs.squeeze() - y_test)\n",
        "    correct = (diff < threshold).sum().item()\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR9YuXfH9h-q",
        "outputId": "08dfaca2-909c-4a3d-c3dd-0d62455d7fd4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 88.13%\n"
          ]
        }
      ]
    }
  ]
}